# toy_opt.mpc
import numpy as np
import torch
from torch import nn,optim
from torch.distributions.multivariate_normal import MultivariateNormal as MN
from torch.utils.data import TensorDataset,DataLoader
from os.path import exists as file_exists
import random
import pdb
import seaborn as sns
import matplotlib.pyplot as plt
import sys
sys.path.insert(0,'/Users/mwerner/Code/cs163_294/dtrust-fl/MP-SPDZ')
from Compiler.types import sfix,sint
from Compiler.compilerLib import Compiler

compiler = Compiler()
class LogisticRegression(nn.Module):
    def __init__(self,input_dim,output_dim):
        super(LogisticRegression,self).__init__()
        self.linear=nn.Linear(input_dim,output_dim)
    def forward(self,x):
        x=self.linear(x)
        return x

def generate_data(switch):
    X_a = torch.tensor(np.random.multivariate_normal(mean=5*np.ones(2),cov=[[1,0],[0,1]],size=int((num_train_samples+num_test_samples)/2)))
    X_b = torch.tensor(np.random.multivariate_normal(mean=1*np.ones(2),cov=[[1,0],[0,1]],size=int((num_train_samples+num_test_samples)/2)))
    y_a = torch.zeros(int((num_train_samples+num_test_samples)/2))
    y_b = torch.ones(int((num_train_samples+num_test_samples)/2))
    
    X = torch.cat((X_a,X_b),axis=0)
    y = torch.cat((y_b,y_a)) if switch else torch.cat((y_a,y_b))

    return X,y

def get_data(client):
    switch=True if client>=int(num_clients/num_clusters) else False
    X,y = generate_data(switch)
    torch.save(X,data_dir+f'client_{client}_X.pkl')
    torch.save(y,data_dir+f'client_{client}_y.pkl')

    X = torch.load(data_dir+f'client_{client}_X.pkl')
    y = torch.load(data_dir+f'client_{client}_y.pkl')

    split_idxs = torch.randperm(num_train_samples+num_test_samples)
    train_idxs = split_idxs[:num_train_samples]
    test_idxs = split_idxs[num_train_samples:]

    train_loader = DataLoader(TensorDataset(X[train_idxs],y[train_idxs]),batch_size=200,shuffle=True)
    test_loader = DataLoader(TensorDataset(X[test_idxs],y[test_idxs]),batch_size=200,shuffle=True)
    return train_loader, test_loader

def compute_acc(model,test_loader):
    model.eval()
    accuracy = []
    for i,(x,y) in enumerate(test_loader):
        with torch.no_grad():
            preds = torch.argmax(model(x.float()), dim=1)
            accuracy += [preds.eq(y),]
    
    accuracy = torch.cat(accuracy, dim=0)
    return accuracy.float().mean().unsqueeze(0)

def select_cluster(test_loader):
    max_acc = torch.tensor([-100])
    for cluster_id in range(num_clusters):
        model = LogisticRegression(data_dim,num_classes)
        model.load_state_dict(torch.load(model_dir+f'model_for_cluster_{cluster_id}.pkl'))
          
        acc = compute_acc(model,test_loader)
        if acc>max_acc:
            max_acc = acc
            selected_cluster_id = cluster_id
            selected_model = model
    selected_model_acc = compute_acc(selected_model,test_loader)
    return selected_model,selected_cluster_id,selected_model_acc

def update_model(model,train_loader):
    optimizer = optim.SGD(model.parameters(),lr=0.001,momentum=0.9,weight_decay=0.0005)
    objective = nn.CrossEntropyLoss()
    for epoch in range(num_epochs):
        train_loss=0.0
        train_accuracy=0.0

        model.train()

        for i,(x,y) in enumerate(train_loader):
            optimizer.zero_grad()
            pred = model(x.float())
            loss = objective(pred,y.long())
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            train_accuracy += (pred.max(1)[1]==y).sum().item()

        train_loss = train_loss/len(train_loader.dataset)
        train_accuracy = train_accuracy/len(train_loader.dataset)
        #print('Epoch:{} \tTraining Loss: {:.6f} Training Acc: {:.6f}'.format(epoch,train_loss,train_accuracy))

    return model

def vectorize_model(model):
    vectorized_model = torch.empty(0)
    for _,param in model.named_parameters():
        vectorized_model = torch.cat((vectorized_model,param.flatten()))
    return vectorized_model

def create_shares(params):
    shares = sfix.Matrix(num_clients,num_params+1)
    for i in range(num_clients):
      shares[i] = params[i]
    shares.write_to_file(0)
    return

def mpc_on_shares(t,selected_clusters):
    params_per_cluster = sfix.Matrix(num_clusters,num_params+1)
    num_clients_per_cluster = sfix.Array(num_clusters) 
    
    params = sfix.Matrix(num_clients,num_params+1)
    params.read_from_file(0)
    
    # average shares per-cluster
    for client in range(num_clients):
      cluster_id = params[client][-1]
      print_ln('cluster %s',cluster_id.reveal())
      @for_range(num_clusters)
      def _(i):
        num_clients_per_cluster[i] = (cluster_id==i).if_else(num_clients_per_cluster[i]+sint(1),num_clients_per_cluster[i])
        params_per_cluster[i] = (cluster_id==i).if_else(params_per_cluster[i][:]+params[client][:],params_per_cluster[i])
    
    # broadcast shares back to clients
    for i in range(num_clusters):
      personalized_model = params_per_cluster[i]
        
      print_ln('personalized model %s',personalized_model.reveal_nested())
            
      # reveal the model 
      f = open(f'Player-Data/Binary-Output-P0-0')
      personalized_model.reveal().binary_output(0)
      vectorized_model = torch.tensor(np.fromfile(f,dtype=np.double))
      vectorized_model = vectorized_model[torch.arange(len(vectorized_model))%(num_params+1)!=num_params]
      #print('vectorized_model',vectorized_model)
      if len(vectorized_model) > 0:
        num_clients_in_cluster = np.unique(selected_clusters,return_counts=True)[1][i]
        torch.save(unvectorize_model(vectorized_model[(2*t+i)*num_params:(2*t+i+1)*num_params]/num_clients_in_cluster).state_dict(), model_dir + f'model_for_cluster_{i}.pkl')
        print(unvectorize_model(vectorized_model[(2*t+i)*num_params:(2*t+i+1)*num_params]).state_dict())
    return

def unvectorize_model(vectorized_model):
    with torch.no_grad():
        model = LogisticRegression(data_dim,num_classes)
        params_shape_dict = {name:torch.tensor(param.shape) for name,param in model.named_parameters()}

        state_dict = model.state_dict()
        current_param_num = 0
        for name,_ in model.named_parameters():
            shape = params_shape_dict[name]
            num_params = torch.prod(shape)
            parameters = vectorized_model[current_param_num:current_param_num+num_params]
            state_dict[name] = torch.reshape(parameters,tuple(shape.tolist()))
            current_param_num += num_params
        model.load_state_dict(state_dict)
    return model

def initialize_cluster_models():
    for cluster in range(num_clusters):
        model = LogisticRegression(data_dim,num_classes)
        torch.save(model.state_dict(),model_dir + f'model_for_cluster_{cluster}.pkl')
    return

@compiler.register_function('secure_pfl')
def secure_pfl():
    initialize_cluster_models()
    personalized_accuracies = torch.zeros((num_clients,T))
    
    # for standard fl
    global_model = LogisticRegression(data_dim,num_classes)
    global_accuracies = torch.zeros((num_clients,T))
    
    for t in range(T):
        params = np.zeros((num_clients,num_params+1))
        
        # for standard fl
        local_models = torch.zeros((num_clients,num_params))
       
        for client in range(num_clients):
            train_loader,test_loader = get_data(client) 
            
            # for standard fl     
            global_accuracies[client][t] = compute_acc(global_model,test_loader)
            local_models[client] = vectorize_model(update_model(global_model,train_loader))
            
            # select cluster/model which incurs lowest loss on test data
            selected_model,selected_cluster_id,selected_model_acc = select_cluster(test_loader)
            personalized_accuracies[client][t] = selected_model_acc
            # perform personalized gradient update
            updated_selected_model = update_model(selected_model,train_loader)
            # store updated model for mpc
            vectorized_model = vectorize_model(updated_selected_model)
            params[client] = torch.cat((vectorized_model,torch.tensor([selected_cluster_id]))).detach().numpy()
        print('params',params.transpose()[-1])
        create_shares(params)
        mpc_on_shares(t,params.transpose()[-1])
        
        # for standard fl
        global_model = unvectorize_model(local_models.mean(axis=0))

    torch.save(personalized_accuracies,data_dir + 'selected_model_accuracies.pkl')
    torch.save(global_accuracies, data_dir + 'global_model_accuracies.pkl')
    print('accuracies',personalized_accuracies,global_accuracies)
    return

def plot_data():
    data = torch.load(data_dir + 'client_0_X.pkl')
    normal_large = data[:int(len(data)/2)]
    normal_small = data[int(len(data)/2):]

    fig,ax = plt.subplots()
    sns.scatterplot(normal_large.t()[0],normal_large.t()[1],color=sns.color_palette('pastel')[0])
    sns.scatterplot(normal_small.t()[0],normal_small.t()[1],color=sns.color_palette('pastel')[3])
    ax.text(0.85, 0.4, '\n'.join(('Client 1: 1', 'Client 2: 1', 'Client 3: 0', 'Client 4: 0')), 
            transform=ax.transAxes,
            fontsize=9,
            verticalalignment='top')
    ax.text(0.1, 0.7, '\n'.join(('Client 1: 0', 'Client 2: 0', 'Client 3: 1', 'Client 4: 1')), 
            transform=ax.transAxes,
            fontsize=10,
            verticalalignment='top')
    ax.set_xlabel('Feature 1',fontsize=10)
    ax.set_ylabel('Feature 2', fontsize=10)
    sns.despine()
    fig.tight_layout()
    plt.savefig('/Users/mwerner/desktop/data.pdf')

def plot_accuracies():
    accuracies = torch.load(data_dir + 'selected_model_accuracies.pkl')
    global_accuracies = torch.load(data_dir + 'global_model_accuracies.pkl')
    fig,axs = plt.subplots(2,1,figsize=(3,4))
    sns.lineplot(np.arange(1,T+1),accuracies[0],ax=axs[0],label='personalized (clients 1,2)',color=sns.color_palette('muted')[6])
    sns.lineplot(np.arange(1,T+1),global_accuracies[0],ax=axs[0],label='non-personalized',color=sns.color_palette('muted')[2])
    sns.lineplot(np.arange(1,T+1),accuracies[2],ax=axs[1],label='personalized (clients 3,4)',color=sns.color_palette('muted')[6])
    sns.lineplot(np.arange(1,T+1),global_accuracies[2],ax=axs[1],label='non-personalized',color=sns.color_palette('muted')[2])
    for i in range(2):
      axs[i].xaxis.set_tick_params(labelsize=6)
      axs[i].yaxis.set_tick_params(labelsize=6)
      axs[i].legend(fontsize=5)
      axs[i].set_xlabel('Round',fontsize=8)
    axs[0].set_ylabel('Pred. Accuracy \n on Client 1\'s test data',fontsize=8)
    axs[1].set_ylabel('Pred. Accuracy \n on Client 3\'s test data',fontsize=8)
    sns.despine()
    fig.tight_layout()
    plt.savefig('/Users/mwerner/desktop/accuracies.pdf')

# fix randomness
np.random.seed(1)
torch.manual_seed(1)
random.seed(1)

# data parameters
data_dir = '/Users/mwerner/code/cs163_294/dtrust-fl/MP-SPDZ/Programs/Source/toy_data/'
model_dir = '/Users/mwerner/code/cs163_294/dtrust-fl/MP-SPDZ/Programs/Source/toy_models/'

# model parameters
num_train_samples = 1000
num_test_samples = 100
num_epochs = 200
batch_size = 200
data_dim = 2
num_classes = 2
num_params = num_classes*(data_dim+1)
sfix.set_precision(16,31)

# fl/mpc parameters
num_clients = 4
num_clusters = 2
T = 4
secure_pfl()
#plot_data()
plot_accuracies()
if __name__=='__main__':
    compiler.compile_func()
