# dtrust_fl_imnet.mpc
import numpy as np
import torch
from torch import nn,optim
from torch.distributions.multivariate_normal import MultivariateNormal as MN
from torch.utils.data import TensorDataset,DataLoader,SubsetRandomSampler
import torchvision
from torchvision.datasets import MNIST
from torchvision.transforms import Compose,Resize,ToTensor
from os.path import exists as file_exists
import random
import pdb
import seaborn as sns
import matplotlib.pyplot as plt
import sys
sys.path.insert(0,'/Users/mwerner/Code/cs163_294/dtrust-fl/MP-SPDZ')
from Compiler.types import sfix,sint
from Compiler.compilerLib import Compiler

compiler = Compiler()
def simclr_model():
  model = resnet18(pretrained=False)
  checkpoint = torch.load(simclr_model_path,map_location=torch.device('cpu'))
  model.load_state_dict(checkpoint['state_dict'],strict=False)
  return model

class Embed(nn.Module):
  def __init__(self,model):
    super(Embed,self).__init__()
    self.embed=nn.Sequential(*list(model.children())[:-1])

  def forward(self,x):
    x=self.embed(x)
    return x

class LogisticRegression(nn.Module):
  def __init__(self,input_dim,output_dim):
    super(LogisticRegression,self).__init__()
    self.linear=nn.Linear(input_dim,output_dim)

  def forward(self,x):
    x=self.linear(x)
    return x

def get_data(client,labels):
  if not file_exists(data_dir+'train_idxs_subsets.pkl'):
    train_idxs = torch.arange(num_train_samples)[torch.randperm(num_train_samples)]
    test_idxs = torch.arange(num_test_samples)[torch.randperm(num_test_samples)]
    train_idxs_subsets = torch.split(train_idxs,int(num_train_samples/num_clients))
    test_idxs_subsets = torch.split(test_idxs,int(num_test_samples/num_clients))
    torch.save(train_idxs_subsets,data_dir+'train_idxs_subsets.pkl')
    torch.save(test_idxs_subsets,data_dir+'test_idxs_subsets.pkl')
  
  train_idxs_subsets = torch.load(data_dir+'train_idxs_subsets.pkl')
  test_idxs_subsets = torch.load(data_dir+'test_idxs_subsets.pkl')

  train_embeds = torch.load(train_embeds_path)
  train_labels = torch.load(train_labels_path)
  test_embeds = torch.load(test_embeds_path)
  test_labels = torch.load(test_labels_path)
  
  train_idx_dict = {i:None for i in range(num_classes)}
  for i in range(num_classes):
    train_idx_dict[i] = (train_labels==i).nonzero().squeeze()
  for i in range(num_classes):
    train_labels.index_fill_(0,train_idx_dict[i],labels[i])
  
  test_idx_dict = {i:None for i in range(num_classes)}
  for i in range(num_classes):
    test_idx_dict[i] = (test_labels==i).nonzero().squeeze()
  for i in range(num_classes):
    test_labels.index_fill_(0,test_idx_dict[i],labels[i])
  
  train_loader = DataLoader(TensorDataset(train_embeds[train_idxs_subsets[client]],train_labels[train_idxs_subsets[client]]),
                            batch_size=batch_size,
                            shuffle=True)

  test_loader = DataLoader(TensorDataset(test_embeds[test_idxs_subsets[client]],test_labels[test_idxs_subsets[client]]),
                           batch_size=batch_size,
                           shuffle=True)
  
  return train_loader, test_loader

def compute_acc(model,test_loader):
    model.eval()
    accuracy = []
    for i,(x,y) in enumerate(test_loader):
        with torch.no_grad():
            preds = torch.argmax(model(x.squeeze().float()), dim=1)
            accuracy += [preds.eq(y),]
    
    accuracy = torch.cat(accuracy, dim=0)
    return accuracy.float().mean().unsqueeze(0)

def select_cluster(test_loader):
    max_acc = torch.tensor([-100])
    for cluster_id in range(num_clusters):
        model = LogisticRegression(data_dim,num_classes)
        model.load_state_dict(torch.load(model_dir+f'model_for_cluster_{cluster_id}.pkl'))
          
        #print('loaded model',cluster_id, model.state_dict())
        acc = compute_acc(model,test_loader)
        print('acc', acc)
        if acc>max_acc:
            max_acc = acc
            selected_cluster_id = cluster_id
            selected_model = model
    selected_model_acc = compute_acc(selected_model,test_loader)
    return selected_model,selected_cluster_id,selected_model_acc

def update_model(model,train_loader):
    optimizer = optim.SGD(model.parameters(),lr=0.001,momentum=0.9,weight_decay=0.0005)
    objective = nn.CrossEntropyLoss()
    for epoch in range(num_epochs):
        train_loss=0.0
        train_accuracy=0.0

        model.train()

        for i,(x,y) in enumerate(train_loader):
            optimizer.zero_grad()
            pred = model(x.squeeze().float())
            loss = objective(pred,y.long())
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            train_accuracy += (pred.max(1)[1]==y).sum().item()

        train_loss = train_loss/len(train_loader.dataset)
        train_accuracy = train_accuracy/len(train_loader.dataset)
        #print('Epoch:{} \tTraining Loss: {:.6f} Training Acc: {:.6f}'.format(epoch,train_loss,train_accuracy))

    return model

def vectorize_model(model):
    vectorized_model = torch.empty(0)
    for _,param in model.named_parameters():
        vectorized_model = torch.cat((vectorized_model,param.flatten()))
    return vectorized_model

def create_shares(params):
    shares = sfix.Matrix(num_clients,num_params+1)
    for i in range(num_clients):
      shares[i] = params[i]
    shares.write_to_file(0)
    return

def mpc_on_shares(t,selected_clusters):
    params_per_cluster = sfix.Matrix(num_clusters,num_params+1)
    num_clients_per_cluster = sfix.Array(num_clusters) 
    
    params = sfix.Matrix(num_clients,num_params+1)
    params.read_from_file(0)
    
    # average shares per-cluster
    for client in range(num_clients):
      cluster_id = params[client][-1]
      print_ln('cluster %s',cluster_id.reveal())
      @for_range(num_clusters)
      def _(i):
        num_clients_per_cluster[i] = (cluster_id==i).if_else(num_clients_per_cluster[i]+sint(1),num_clients_per_cluster[i])
        params_per_cluster[i] = (cluster_id==i).if_else(params_per_cluster[i][:]+params[client][:],params_per_cluster[i])
    
    # broadcast shares back to clients
    personalized_model_0 = params_per_cluster[0]
    f_0 = open(f'Player-Data/Binary-Output-P0-0')
    personalized_model_0.reveal().binary_output(0)
    vectorized_model_0 = torch.tensor(np.fromfile(f_0,dtype=np.double))
    vectorized_model_0 = vectorized_model_0[torch.arange(len(vectorized_model_0))%(num_params+1)!=num_params]
    
    file_to_delete = open('Player-Data/Binary-Output-P0-0','w')
    file_to_delete.close()
    personalized_model_1 = params_per_cluster[1]
    f_1 = open(f'Player-Data/Binary-Output-P0-0')
    personalized_model_1.reveal().binary_output(0)
    vectorized_model_1 = torch.tensor(np.fromfile(f_1,dtype=np.double))
    print_ln('personalized model %s',personalized_model_1.reveal_nested())
    print_ln('personalized model %s',personalized_model_0.reveal_nested())
    vectorized_model_1 = vectorized_model_1[torch.arange(len(vectorized_model_1))%(num_params+1)!=num_params]
    print(vectorized_model_1,vectorized_model_0)
    return
    #for i in range(num_clusters):
    #  personalized_model = params_per_cluster[i]
    #    
    #  print_ln('personalized model %s',personalized_model.reveal_nested())
    #        
    #  return
    #  # reveal the model 
    #  f = open(f'Player-Data/Binary-Output-P0-0')
    #  personalized_model.reveal().binary_output(0)
    #  vectorized_model = torch.tensor(np.fromfile(f,dtype=np.double))
    #  vectorized_model = vectorized_model[torch.arange(len(vectorized_model))%(num_params+1)!=num_params]
    #  #print('vectorized_model',vectorized_model)
    if len(vectorized_model_0) > 0:
      num_clients_in_cluster_0 = np.unique(selected_clusters,return_counts=True)[1][0]
      num_clients_in_cluster_1 = np.unique(selected_clusters,return_counts=True)[1][1]
      #torch.save(unvectorize_model(vectorized_model_0[(2*t+i)*num_params:(2*t+i+1)*num_params]/num_clients_in_cluster).state_dict(), model_dir + f'model_for_cluster_0.pkl')
      torch.save(unvectorize_model(vectorized_model_0[(2*t)*num_params:(2*t+1)*num_params]/num_clients_in_cluster_0).state_dict(), model_dir + f'model_for_cluster_0.pkl')
      torch.save(unvectorize_model(vectorized_model_1[(2*t)*num_params:(2*t+1)*num_params]/num_clients_in_cluster_1).state_dict(), model_dir + f'model_for_cluster_1.pkl')
      print(unvectorize_model(vectorized_model_0[(2*t)*num_params:(2*t+1)*num_params]).state_dict())
      print(unvectorize_model(vectorized_model_1[(2*t)*num_params:(2*t+1)*num_params]).state_dict())
    return

def unvectorize_model(vectorized_model):
    with torch.no_grad():
        model = LogisticRegression(data_dim,num_classes)
        params_shape_dict = {name:torch.tensor(param.shape) for name,param in model.named_parameters()}

        state_dict = model.state_dict()
        current_param_num = 0
        for name,_ in model.named_parameters():
            shape = params_shape_dict[name]
            num_params = torch.prod(shape)
            parameters = vectorized_model[current_param_num:current_param_num+num_params]
            state_dict[name] = torch.reshape(parameters,tuple(shape.tolist()))
            current_param_num += num_params
        model.load_state_dict(state_dict)
    return model

def initialize_models():
    #if not file_exists(model_dir + f'model_for_cluster_0.pkl'):
    for cluster in range(num_clusters):
        model = LogisticRegression(data_dim,num_classes)
        torch.save(model.state_dict(),model_dir + f'model_for_cluster_{cluster}.pkl')
    return

def save_model(model,path):
  torch.save(model.state_dict(),path)

def load_model(path):
  model = LogisticRegression(data_dim,num_classes)
  model.load_state_dict(torch.load(path))
  return model

@compiler.register_function('run_personalized_fl')
def run_personalized_fl():
    accuracies = torch.zeros((num_clients,T))
    global_accuracies = torch.zeros((num_clients,T))
    initialize_models()
    global_model = LogisticRegression(data_dim,num_classes)
    #save_model(global_model,model_dir + 'global_model.pkl')
    label_perm = torch.randperm(num_classes)
    for t in range(T):
        params = np.zeros((num_clients,num_params+1))
        local_models = torch.zeros((num_clients,num_params))
        for client in range(num_clients):
            labels = torch.arange(num_classes) if client <= int(num_clients/num_clusters) else label_perm
            labels = torch.arange(num_classes) if client ==0 else label_perm
            train_loader,test_loader = get_data(client,labels) 
            
            #trained_model = update_model(current_global_model,test_loader)
            #print(f'client {client} accuracy {compute_acc(trained_model,test_loader)}')
            #global_accuracies[client][t] = compute_acc(load_model(model_dir+'global_model.pkl'),test_loader)
            #local_models[client] = vectorize_model(update_model(load_model(model_dir+'global_model.pkl'),train_loader))
            local_models[client] = vectorize_model(update_model(global_model,train_loader))
            selected_model,selected_cluster_id,selected_model_acc = select_cluster(test_loader)
    #        accuracies[client][t] = selected_model_acc
            updated_selected_model = update_model(selected_model,train_loader)
            print(f'individual accuracy', compute_acc(updated_selected_model,test_loader))
            #save_model(updated_selected_model,model_dir + f'model_for_cluster_{selected_cluster_id}.pkl')
            vectorized_model = vectorize_model(updated_selected_model)
            params[client] = torch.cat((vectorized_model,torch.tensor([selected_cluster_id]))).detach().numpy()
            print('params',params.transpose()[-1])
        create_shares(params)
        mpc_on_shares(t,params.transpose()[-1])
        #print('local_models', local_models)
        global_model = unvectorize_model(local_models.mean(axis=0))
        #torch.save(global_model.state_dict(),model_dir+'global_model.pkl')
    #print(f'global accuracy',compute_acc(load_model(model_dir + 'global_model.pkl'),get_data(0,torch.arange(num_classes))[1]))
    #print(f'global accuracy',compute_acc(load_model(model_dir + 'global_model.pkl'),get_data(1,torch.arange(num_classes))[1]))
    #print(f'global accuracy',compute_acc(load_model(model_dir + 'global_model.pkl'),get_data(2,label_perm)[1]))
    #print(f'global accuracy',compute_acc(load_model(model_dir + 'global_model.pkl'),get_data(3,label_perm)[1]))
    #print(f'cluster 1 accuracy',compute_acc(load_model(model_dir + 'model_for_cluster_1.pkl'),get_data(0,torch.arange(num_classes))[1]))
    #print(f'cluster 1 accuracy',compute_acc(load_model(model_dir + 'model_for_cluster_1.pkl'),get_data(1,torch.arange(num_classes))[1]))
    #print(f'cluster 0 accuracy',compute_acc(load_model(model_dir + 'model_for_cluster_0.pkl'),get_data(2,label_perm)[1]))
    #print(f'cluster 0 accuracy', compute_acc(load_model(model_dir + 'model_for_cluster_0.pkl'),get_data(3,label_perm)[1]))
    torch.save(accuracies,data_dir + 'selected_model_accuracies.pkl')
    torch.save(global_accuracies, data_dir + 'global_model_accuracies.pkl')
    return

def plot_data():
    data = torch.load(data_dir + 'client_0_X_test.pkl')
    normal_large = data[:int(len(data)/2)]
    normal_small = data[int(len(data)/2):]

    fig,ax = plt.subplots()
    sns.scatterplot(normal_large.t()[0],normal_large.t()[1],color=sns.color_palette('pastel')[0])
    sns.scatterplot(normal_small.t()[0],normal_small.t()[1],color=sns.color_palette('pastel')[3])
    ax.text(0.85, 0.4, '\n'.join(('Client 1:+', 'Client 2:+', 'Client 3: -', 'Client 4: -')), 
            transform=ax.transAxes,
            fontsize=9,
            verticalalignment='top')
    ax.text(0.1, 0.7, '\n'.join(('Client 1: -', 'Client 2: -', 'Client 3:+', 'Client 4:+')), 
            transform=ax.transAxes,
            fontsize=10,
            verticalalignment='top')
    ax.set_xlabel('Feature 1',fontsize=10)
    ax.set_ylabel('Feature 2', fontsize=10)
    sns.despine()
    fig.tight_layout()
    plt.savefig('/Users/mwerner/desktop/data.pdf')

def plot_accuracies():
    accuracies = torch.load(data_dir + 'selected_model_accuracies.pkl')
    global_accuracies = torch.load(data_dir + 'global_model_accuracies.pkl')
    fig,axs = plt.subplots(1,2,figsize=(4,2))
    sns.lineplot(np.arange(1,T+1),accuracies[0],ax=axs[0],label='personalized (clients 1,2)',color=sns.color_palette('muted')[6])
    sns.lineplot(np.arange(1,T+1),global_accuracies[0],ax=axs[0],label='non-personalized',color=sns.color_palette('muted')[2])
    sns.lineplot(np.arange(1,T+1),accuracies[2],ax=axs[1],label='personalized (clients 3,4)',color=sns.color_palette('muted')[6])
    sns.lineplot(np.arange(1,T+1),global_accuracies[2],ax=axs[1],label='non-personalized',color=sns.color_palette('muted')[2])
    for i in range(2):
      axs[i].xaxis.set_tick_params(labelsize=6)
      axs[i].yaxis.set_tick_params(labelsize=6)
      axs[i].legend(fontsize=5)
      axs[i].set_xlabel('Round',fontsize=8)
    axs[0].set_ylabel('Pred. Accuracy \n on Client 1\'s test data',fontsize=8)
    axs[1].set_ylabel('Pred. Accuracy \n on Client 3\'s test data',fontsize=8)
    sns.despine()
    fig.tight_layout()
    plt.savefig('/Users/mwerner/desktop/accuracies.pdf')

# fix randomness
np.random.seed(1)
torch.manual_seed(1)
random.seed(1)

# data parameters
home_dir = '/Users/mwerner/code/cs163_294/dtrust-fl/MP-SPDZ/Programs/Source'
data_dir = f'{home_dir}/imnet_data/'
model_dir = f'{home_dir}/imnet_models/'

#split_shares_files = list(map(lambda server: f'Persistence/Transactions-P{server}.data',servers))
combine_shares_file = f'Player-Data/Binary-Output-P0-0'
train_embeds_path = f'{home_dir}/imnet_files/embeddings/embed_vectors.pkl' 
train_labels_path = f'{home_dir}/imnet_files/embeddings/embed_labels.pkl' 
test_embeds_path = f'{home_dir}/imnet_files/embeddings/test_embed_vectors.pkl' 
test_labels_path = f'{home_dir}/imnet_files/embeddings/test_embed_labels.pkl' 

# model parameters
num_train_samples = 1000
num_test_samples = 500
num_epochs = 50
batch_size = 200
data_dim = 2048
num_classes = 10
num_params = num_classes*(data_dim+1)
sfix.set_precision(16,31)

# fl/mpc parameters
num_clients = 4
num_clusters = 2
T = 1

run_personalized_fl()
#plot_data()
#plot_accuracies()
if __name__=='__main__':
    compiler.compile_func()
